#!/usr/bin/env python
#coding=utf-8

import sys
import argparse,textwrap

#解决中文帮助乱码
#reload(sys)
#print sys.getdefaultencoding()
#sys.setdefaultencoding('utf-8')

class SpiderOption(object):
	def __init__(self):
		#USAGE='usage: %prog [option] args\n%prog -u url -d deep -f logfile -l loglevel(1-5)  --testself -thread number --dbfile  filepath  --key=”HTML5”'
		self.parser=argparse.ArgumentParser(
				#prog='spider.py',
				formatter_class=argparse.RawDescriptionHelpFormatter,
				description=textwrap.dedent('''\
						爬虫工具使用文档
						----------------
						'''),
				#epilog='感谢您的使用^_^',
			)
		self.add_arg()
	def add_arg(self):
		self.parser.add_argument('-u', required=True, dest='url', metavar='url', help='指定爬虫开始的地址')
		self.parser.add_argument('-d', type=int, required=True, dest='deep', metavar='deep', help='指定爬虫深度')
		self.parser.add_argument('--key', help='页面内的关键词，获取满足该关键词的网  页，可选参数，默认为所有页面')
		self.parser.add_argument('-f', default='spider.log', dest='logfile', metavar='logfile', help='指定保存日志文件，(default:%(default)s)')
		self.parser.add_argument('-l', type=int, choices=xrange(1,6), dest='loglevel', metavar='loglevel', help='日志记录文件记录详细程度，数字越大记  录越详细,loglevel(1-5)')
		self.parser.add_argument('--dbfile', metavar='filepath', help='存放结果数据到指定的数据库（sqlite）文  件中')
		self.parser.add_argument('--thread', type=int, default=10, metavar='number', help='指定线程池大小，多线程爬取页面，可选参数，(default:%(default)s)')
		self.parser.add_argument('--testself', help='程序自测，可选参数')
		self.parser.add_argument('-v', '--version', action='version', version='%(prog)s 1.0', help='程序版本信息')
		

	def test(self):
		self.parser.print_help()

if __name__=='__main__':
	SpiderOption().test()
